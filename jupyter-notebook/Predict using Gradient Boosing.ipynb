{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install gensim\n",
    "!pip install SPARQLWrapper\n",
    "!pip install xlrd==1.2.0\n",
    "!pip install nltk\n",
    "!pip install python-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import CRFTagger\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "stopword_set = set(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 14 columns):\n",
      "profile_background_color        1 non-null int64\n",
      "profile_link_color              1 non-null object\n",
      "profile_sidebar_border_color    1 non-null object\n",
      "profile_sidebar_fill_color      1 non-null object\n",
      "profile_text_color              1 non-null object\n",
      "bio                             1 non-null object\n",
      "name                            1 non-null object\n",
      "username                        1 non-null object\n",
      "followers                       1 non-null int64\n",
      "friends_count                   1 non-null int64\n",
      "listed_count                    1 non-null int64\n",
      "favourites_count                1 non-null int64\n",
      "statuses_count                  1 non-null int64\n",
      "protected                       1 non-null bool\n",
      "dtypes: bool(1), int64(6), object(7)\n",
      "memory usage: 185.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_background_color</th>\n",
       "      <th>profile_link_color</th>\n",
       "      <th>profile_sidebar_border_color</th>\n",
       "      <th>profile_sidebar_fill_color</th>\n",
       "      <th>profile_text_color</th>\n",
       "      <th>bio</th>\n",
       "      <th>name</th>\n",
       "      <th>username</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>protected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>352726</td>\n",
       "      <td>D02B55</td>\n",
       "      <td>829D5E</td>\n",
       "      <td>99CC33</td>\n",
       "      <td>3E4415</td>\n",
       "      <td>Governor of West Java. Broadcaster of Daily Ha...</td>\n",
       "      <td>Ridwan Kamil</td>\n",
       "      <td>ridwankamil</td>\n",
       "      <td>4974848</td>\n",
       "      <td>2842</td>\n",
       "      <td>1547</td>\n",
       "      <td>22037</td>\n",
       "      <td>43924</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   profile_background_color profile_link_color profile_sidebar_border_color  \\\n",
       "0                    352726             D02B55                       829D5E   \n",
       "\n",
       "  profile_sidebar_fill_color profile_text_color  \\\n",
       "0                     99CC33             3E4415   \n",
       "\n",
       "                                                 bio          name  \\\n",
       "0  Governor of West Java. Broadcaster of Daily Ha...  Ridwan Kamil   \n",
       "\n",
       "      username  followers  friends_count  listed_count  favourites_count  \\\n",
       "0  ridwankamil    4974848           2842          1547             22037   \n",
       "\n",
       "   statuses_count  protected  \n",
       "0           43924      False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60 entries, 0 to 59\n",
      "Data columns (total 36 columns):\n",
      "id                 60 non-null int64\n",
      "conversation_id    60 non-null int64\n",
      "created_at         60 non-null object\n",
      "date               60 non-null object\n",
      "time               60 non-null object\n",
      "timezone           60 non-null int64\n",
      "user_id            60 non-null int64\n",
      "username           60 non-null object\n",
      "name               60 non-null object\n",
      "place              0 non-null float64\n",
      "tweet              60 non-null object\n",
      "language           60 non-null object\n",
      "mentions           60 non-null object\n",
      "urls               60 non-null object\n",
      "photos             60 non-null object\n",
      "replies_count      60 non-null int64\n",
      "retweets_count     60 non-null int64\n",
      "likes_count        60 non-null int64\n",
      "hashtags           60 non-null object\n",
      "cashtags           60 non-null object\n",
      "link               60 non-null object\n",
      "retweet            60 non-null bool\n",
      "quote_url          0 non-null float64\n",
      "video              60 non-null int64\n",
      "thumbnail          50 non-null object\n",
      "near               0 non-null float64\n",
      "geo                0 non-null float64\n",
      "source             0 non-null float64\n",
      "user_rt_id         0 non-null float64\n",
      "user_rt            0 non-null float64\n",
      "retweet_id         0 non-null float64\n",
      "reply_to           60 non-null object\n",
      "retweet_date       0 non-null float64\n",
      "translate          0 non-null float64\n",
      "trans_src          0 non-null float64\n",
      "trans_dest         0 non-null float64\n",
      "dtypes: bool(1), float64(12), int64(8), object(15)\n",
      "memory usage: 16.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>...</th>\n",
       "      <th>geo</th>\n",
       "      <th>source</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1545938039058604032</td>\n",
       "      <td>1545937935606116352</td>\n",
       "      <td>2022-07-10 09:08:45 WITA</td>\n",
       "      <td>2022-07-10</td>\n",
       "      <td>09:08:45</td>\n",
       "      <td>800</td>\n",
       "      <td>80323736</td>\n",
       "      <td>ridwankamil</td>\n",
       "      <td>Ridwan Kamil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1545937935606116352</td>\n",
       "      <td>1545937935606116352</td>\n",
       "      <td>2022-07-10 09:08:20 WITA</td>\n",
       "      <td>2022-07-10</td>\n",
       "      <td>09:08:20</td>\n",
       "      <td>800</td>\n",
       "      <td>80323736</td>\n",
       "      <td>ridwankamil</td>\n",
       "      <td>Ridwan Kamil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1545371884703072256</td>\n",
       "      <td>1545371884703072256</td>\n",
       "      <td>2022-07-08 19:39:03 WITA</td>\n",
       "      <td>2022-07-08</td>\n",
       "      <td>19:39:03</td>\n",
       "      <td>800</td>\n",
       "      <td>80323736</td>\n",
       "      <td>ridwankamil</td>\n",
       "      <td>Ridwan Kamil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1544966777628925952</td>\n",
       "      <td>1544966764001640449</td>\n",
       "      <td>2022-07-07 16:49:18 WITA</td>\n",
       "      <td>2022-07-07</td>\n",
       "      <td>16:49:18</td>\n",
       "      <td>800</td>\n",
       "      <td>80323736</td>\n",
       "      <td>ridwankamil</td>\n",
       "      <td>Ridwan Kamil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1544966767956869120</td>\n",
       "      <td>1544966764001640449</td>\n",
       "      <td>2022-07-07 16:49:16 WITA</td>\n",
       "      <td>2022-07-07</td>\n",
       "      <td>16:49:16</td>\n",
       "      <td>800</td>\n",
       "      <td>80323736</td>\n",
       "      <td>ridwankamil</td>\n",
       "      <td>Ridwan Kamil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id      conversation_id                created_at  \\\n",
       "0  1545938039058604032  1545937935606116352  2022-07-10 09:08:45 WITA   \n",
       "1  1545937935606116352  1545937935606116352  2022-07-10 09:08:20 WITA   \n",
       "2  1545371884703072256  1545371884703072256  2022-07-08 19:39:03 WITA   \n",
       "3  1544966777628925952  1544966764001640449  2022-07-07 16:49:18 WITA   \n",
       "4  1544966767956869120  1544966764001640449  2022-07-07 16:49:16 WITA   \n",
       "\n",
       "         date      time  timezone   user_id     username          name  place  \\\n",
       "0  2022-07-10  09:08:45       800  80323736  ridwankamil  Ridwan Kamil    NaN   \n",
       "1  2022-07-10  09:08:20       800  80323736  ridwankamil  Ridwan Kamil    NaN   \n",
       "2  2022-07-08  19:39:03       800  80323736  ridwankamil  Ridwan Kamil    NaN   \n",
       "3  2022-07-07  16:49:18       800  80323736  ridwankamil  Ridwan Kamil    NaN   \n",
       "4  2022-07-07  16:49:16       800  80323736  ridwankamil  Ridwan Kamil    NaN   \n",
       "\n",
       "     ...     geo source user_rt_id user_rt retweet_id  reply_to  retweet_date  \\\n",
       "0    ...     NaN    NaN        NaN     NaN        NaN        []           NaN   \n",
       "1    ...     NaN    NaN        NaN     NaN        NaN        []           NaN   \n",
       "2    ...     NaN    NaN        NaN     NaN        NaN        []           NaN   \n",
       "3    ...     NaN    NaN        NaN     NaN        NaN        []           NaN   \n",
       "4    ...     NaN    NaN        NaN     NaN        NaN        []           NaN   \n",
       "\n",
       "   translate trans_src trans_dest  \n",
       "0        NaN       NaN        NaN  \n",
       "1        NaN       NaN        NaN  \n",
       "2        NaN       NaN        NaN  \n",
       "3        NaN       NaN        NaN  \n",
       "4        NaN       NaN        NaN  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "\n",
    "path = '../example-data/'\n",
    "df_prof = pd.read_excel(path + 'df_structured.xlsx')\n",
    "df_prof.info()\n",
    "display(df_prof.head())\n",
    "\n",
    "df_tweet = pd.read_csv(path + 'ridwankamil.csv')\n",
    "df_tweet = df_tweet.drop_duplicates(subset=['tweet'])\n",
    "df_tweet = df_tweet.sort_values(by='created_at', ascending=False)\n",
    "df_tweet.info()\n",
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer\n",
    "\n",
    "path = '../vectorizer/'\n",
    "name_feat_ia_vectorizer = pickle.load(open(path + 'name_feat_ia_vec.pickle', 'rb'))\n",
    "username_feat_char_vectorizer = pickle.load(open(path + 'username_feat_char_vec.pickle', 'rb'))\n",
    "tweet_feat_bow_stop_vectorizer = pickle.load(open(path + 'tweet_feat_bow_stop_vec.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf tagger\n",
    "\n",
    "path = '../misc/'\n",
    "ct = CRFTagger()\n",
    "ct.set_model_file(path + 'all_indo_man_tag_corpus_model.crf.tagger')\n",
    "\n",
    "# emoticon\n",
    "\n",
    "f_open = open(path + 'EMOTICON.txt', 'r')\n",
    "emoticons = f_open.read().split('\\n')\n",
    "f_open.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_dict(endpoint_url, query):\n",
    "    user_agent = 'WDQS-example Python/%s.%s' % (sys.version_info[0], sys.version_info[1])\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "def get_name_tokens(df, gender):\n",
    "    df_temp = df[df.gender == gender]\n",
    "    arr_res = []\n",
    "    for d in df_temp['nama']:\n",
    "        tokens = nltk.word_tokenize(d)\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        arr_res = arr_res + tokens\n",
    "    return list(set(arr_res))\n",
    "\n",
    "def get_vec(vectorizer, arr_text):\n",
    "    return vectorizer.transform(arr_text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All color: [[1 1 1 6 1 2 4 4 2 4 6 1 1 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# extraction\n",
    "\n",
    "color_feat = []\n",
    "\n",
    "color_cols = [\n",
    "    'profile_background_color',\n",
    "    'profile_link_color',\n",
    "    'profile_sidebar_border_color',\n",
    "    'profile_sidebar_fill_color',\n",
    "    'profile_text_color'\n",
    "]\n",
    "\n",
    "for index, row in df_prof.iterrows():\n",
    "    pbc = str(row['profile_background_color'])\n",
    "    plc = str(row['profile_link_color'])\n",
    "    psbc = str(row['profile_sidebar_border_color'])\n",
    "    psfc = str(row['profile_sidebar_fill_color'])\n",
    "    ptc = str(row['profile_text_color'])\n",
    "    \n",
    "    rgb1 = [int(pbc[0:2], 16) // 32, int(pbc[2:4], 16) // 32, int(pbc[4:6], 16) // 32]\n",
    "    rgb2 = [int(plc[0:2], 16) // 32, int(plc[2:4], 16) // 32, int(plc[4:6], 16) // 32]\n",
    "    rgb3 = [int(psbc[0:2], 16) // 32, int(psbc[2:4], 16) // 32, int(psbc[4:6], 16) // 32]\n",
    "    rgb4 = [int(psfc[0:2], 16) // 32, int(psfc[2:4], 16) // 32, int(psfc[4:6], 16) // 32]\n",
    "    rgb5 = [int(ptc[0:2], 16) // 32, int(ptc[2:4], 16) // 32, int(ptc[4:6], 16) // 32]\n",
    "    \n",
    "    color_feat.append(sum([rgb1, rgb2, rgb3, rgb4, rgb5], []))\n",
    "\n",
    "color_feat = np.array(color_feat)\n",
    "print('All color:', color_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "laki-laki                24742\n",
       "perempuan                12391\n",
       "perempuan transgender        7\n",
       "Name: gender, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>nama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>laki-laki</td>\n",
       "      <td>Triyatno</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender      nama\n",
       "0  laki-laki  Triyatno"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ghozali', 'assegaf', 'najib', 'katoppo', 'mochamad']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['jumah', 'fitrika', 'aifan', 'assegaf', 'umami']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get name dictionary\n",
    "\n",
    "endpoint_url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "query = '''SELECT ?item ?itemLabel ?sexLabel\n",
    "WHERE {\n",
    "  ?item wdt:P31 wd:Q5 .\n",
    "  ?item wdt:P27 wd:Q252 .\n",
    "  ?item wdt:P21 ?sex .\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"id, en\". }\n",
    "}'''\n",
    "\n",
    "results = get_wikidata_dict(endpoint_url, query)\n",
    "\n",
    "df_wikidata = pd.DataFrame()\n",
    "for index, row in pd.DataFrame(results['results']['bindings']).iterrows():\n",
    "    nama = row['itemLabel']['value']\n",
    "    gender = row['sexLabel']['value']\n",
    "\n",
    "    df_wikidata = df_wikidata.append({\n",
    "        'nama': nama,\n",
    "        'gender': gender\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "df_wikidata = df_wikidata.drop_duplicates()\n",
    "display(df_wikidata['gender'].value_counts(dropna=False))\n",
    "display(df_wikidata.head(1))\n",
    "\n",
    "tokens_pria = get_name_tokens(df_wikidata, 'laki-laki')\n",
    "print(len(tokens_pria))\n",
    "display(tokens_pria[0:5])\n",
    "\n",
    "tokens_wanita = get_name_tokens(df_wikidata, 'perempuan')\n",
    "print(len(tokens_wanita))\n",
    "display(tokens_wanita[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ridwan kamil'] [[2 0]]\n"
     ]
    }
   ],
   "source": [
    "# extraction\n",
    "\n",
    "arr_name = []\n",
    "dict_name = []\n",
    "\n",
    "for d in df_prof['name']:\n",
    "    d = d.lower()\n",
    "    tokens = nltk.word_tokenize(d)\n",
    "    \n",
    "    contains_stopword = len(set(tokens) & (stopword_set - set(['dini', 'hari']))) > 0\n",
    "    if re.sub(r'[\\.\\s\\'\\_\\(\\)\\-\\|\\/\\@]', '', d).isalpha() and not contains_stopword:\n",
    "        arr_name.append(d)\n",
    "\n",
    "        count_pria = 0\n",
    "        count_wanita = 0 \n",
    "\n",
    "        for t in range(len(tokens)):            \n",
    "            if tokens[t] in tokens_pria:\n",
    "                count_pria += 1\n",
    "            elif tokens[t] in tokens_wanita:\n",
    "                count_wanita += 1\n",
    "        dict_name.append([count_pria, count_wanita])\n",
    "        \n",
    "    else:\n",
    "        arr_name.append('')\n",
    "        dict_name.append([0, 0])\n",
    "        \n",
    "print(np.array(arr_name) , np.array(dict_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n",
      "[[0 0 0 ... 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# vectorization\n",
    "\n",
    "ia = get_vec(name_feat_ia_vectorizer, arr_name)\n",
    "\n",
    "print(ia)\n",
    "\n",
    "# concate with name dict feature\n",
    "\n",
    "ia_with_dict = np.hstack((ia, dict_name))\n",
    "\n",
    "print(ia_with_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# vectorization\n",
    "\n",
    "arr_ids = np.array([re.sub(r'\\d+', '', d) for d in df_prof['username']])\n",
    "username_feat_char = get_vec(username_feat_char_vectorizer, arr_ids)\n",
    "\n",
    "print(username_feat_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([4.974848e+06, 2.842000e+03, 1.547000e+03, 2.203700e+04,\n",
      "       1.000000e+00])]\n"
     ]
    }
   ],
   "source": [
    "retweets_count = df_tweet[df_tweet['retweets_count'] > 0].shape[0] / df_tweet.shape[0]\n",
    "\n",
    "network_feat_abl_likes = [np.hstack((df_prof['followers'], df_prof['friends_count'], df_prof['listed_count'], df_prof['favourites_count'], retweets_count))]\n",
    "print(network_feat_abl_likes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.41666667, 0.41666667, 0.83333333, 2.85714286])]\n"
     ]
    }
   ],
   "source": [
    "# extraction\n",
    "\n",
    "hashtags = df_tweet[df_tweet.hashtags != '[]'].shape[0] / df_tweet.shape[0]\n",
    "photos = df_tweet[df_tweet.photos != '[]'].shape[0] / df_tweet.shape[0]\n",
    "video = df_tweet[df_tweet.video == 1].shape[0] / df_tweet.shape[0]\n",
    "days = df_tweet.groupby('date')['tweet'].nunique().mean()\n",
    "\n",
    "behavior_feat_abl_mention = [np.hstack((hashtags, photos, video, days))]\n",
    "print(behavior_feat_abl_mention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2503, 15250,     0,     0,     0,    39,   437,     0,    68,\n",
      "         596])]\n"
     ]
    }
   ],
   "source": [
    "# extraction\n",
    "\n",
    "word_count_sum = 0\n",
    "char_count_sum = 0\n",
    "repeated_alphabets_sum = 0\n",
    "ellipses_sum = 0\n",
    "exclamation_sum = 0\n",
    "upper_words_sum = 0\n",
    "capitalized_words_sum = 0 \n",
    "emoticon_sum = 0\n",
    "adjective_sum = 0\n",
    "noun_sum = 0\n",
    "\n",
    "for t in df_tweet['tweet']:\n",
    "    token = nltk.word_tokenize(t)\n",
    "    word_count_sum += len(token)\n",
    "    char_count_sum += len(t)\n",
    "    repeated_alphabets_sum += len(re.findall(r'([A-Za-z])\\1\\1', t))\n",
    "    ellipses_sum += len(re.findall(r'([.])\\1', t))\n",
    "    exclamation_sum += len(re.findall(r'([!])\\1', t))\n",
    "\n",
    "    for w in token:\n",
    "        if w.isupper():\n",
    "            upper_words_sum += 1\n",
    "        if w.istitle():\n",
    "            capitalized_words_sum += 1\n",
    "\n",
    "    for e in emoticons:\n",
    "        emoticon_sum += t.count(e)\n",
    "\n",
    "    tag = ct.tag_sents([token])\n",
    "    flat_tag = [item for sublist in tag for item in sublist]\n",
    "    pos_count = Counter([j for i, j in flat_tag])\n",
    "    adjective_sum += pos_count['JJ']\n",
    "    noun_sum += pos_count['NN']\n",
    "    \n",
    "socio_feat_abl_verb_count = [np.hstack((word_count_sum, char_count_sum, repeated_alphabets_sum, ellipses_sum, exclamation_sum, upper_words_sum, capitalized_words_sum, emoticon_sum, adjective_sum, noun_sum))]\n",
    "print(socio_feat_abl_verb_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"semoga qurban pengorbanan diterima allah swt pahala menyelamatkan kelak . haturkan mohon maaf lahir bathin . hatur nuhun . https : //t.co/bf9obneb2p alhamdulillah , proses badal haji menghajikan nama eril tunaikan . niat proses wajib rukun sunnahnya . semoga haji mabrur . selamat idul adha 1443 h , haturkan umat islam merayakan . mohon maaf lahir batin . https : //t.co/cnk7ka0xqy suasana indonesia tanah suci . mengecek kondisi jamaah , serasa kampung , mekkah berjumpa dirindukan melebihi istri the one and only : bala-bala . # jabarjuara # indonesiajuara https : //t.co/i9vec88cjl insya allah , pulang tanah air predikat hajjah menyempurnakan rukun islam ke-5 nya . bersyukurlah keberkahan . bersabarlah sat kemusibahan . berikhtiarlah mengejar tujuan berdoalah perlindungan . https : //t.co/5s37tpnuo6 untungnya , islam kemudahan rukhsah berhalangan . tim haji jawa barat mengantisipasi hal-hal , personel pengganti winingsih syariat badal haji dihajikan . menyemangati winingsih , salah jemaah haji subang jawa barat yg ujian . jenguk & amp ; semangati semalam . beliau sakit terkena stroke tanah suci yg membuatnya bergerak melaksanakan lanjutan ritual ibadah haji . https : //t.co/exieit1fvr memahami betapa susahnya kesempatan berhaji usia , dana raga , doakan berniat , allah mudahkan kabulkan cita-citanya berhaji tanah suci secepatnya . aamiin . kemarin inspeksi , menyapa , menyemangati mendoakan jamaah haji sehat , fokus ibadah , pergi pulang selamat gelar haji mabrur . https : //t.co/typlm4mwjf 2 . sistem organisasi lengkap , apapun tim khusus menangani . 3 . dapur/catering makanan khas indonesia 3 kali sehari jamaah betah . 4 . kamar nyaman bersih ber-3 ber-4 . aman nyaman terkendali . sistem pelayanan jemaah haji jawa barat dimaksimalkan . 1 . klinik kesehatan hotel sektor . jemaah gedor-gedor kamar dokter . https : //t.co/kw1hsoqzxq mohon doanya kelancaran jemaah haji jawa barat indonesia lancar sehat selamat tanah air haji mabrur hajjah mabrurah . aamiin . hatur nuhun . berhaji nama eril . besok senin sbg gubernur , pergi menunaikan tugas memimpin jemaah haji jabar yg 17,000-an jemaah . momen , berhaji nama almarhum emmeril kahn mumtadz . , pagi ziarah , pamit berdoa makam eril . https : //t.co/tpjrwjxdg8 progres masjid agung jawa barat . nama masjid al jabbar . doakan lancar 2 tertunda pandemi covid . insya allah , selesai , & amp ; kebanggaan masyarakat jabar . hidup , menua karya inspirasi . # jabarjuara https : //t.co/tqdxykpvno progres masjid agung jawa barat , nama masjid al jabbar . doakan lancar 2 tertunda pandemi covid . insya allah , selsai kebanggaan masyarakat jawa barat . # jabarjuara # indonesiajuara https : //t.co/jtyxr7vshm kirimkan doa terbaik alfatihah . semoga ilmu , nasehat terima keteladanan almarhum bpk tjahjo kumolo saksikan inspirasi . aamiin . innailaihi wa inna lillahi rajiun . pemprov jawa barat beserta jajaran berduka cita sedalam-dalamnya berpulangnya @ tjahjo_kumolo menteri pan rb ri . semoga diterima iman islamnya , keluarga yg ditinggalkan ketabahan & amp ; kekuatan . aamiin . https : //t.co/tlcsjv02mh menemukan warga jabar tinggal layak , kontak @ dinsosjabar @ qrjabar . hatur nuhun . https : //t.co/ueygstuqip bu ami dilamar ridwan kamil ? . hadiah utk bu ami berbentuk cincin sesuai keinginannya , rumahnya yg bilik reyot beralas tanah skrg bangun ulang . beliau sebatang kara , mengandalkan hidup dgn berdagang minuman sachet . # jabarjuara https : //t.co/5d0yd6hrkd paginya melepas kontingen jabar mengikuti festival olahraga rekreasi masyarakat palembang , pesan tim jabar juara sportif profesional . mari semangat membangun yg maju . # jabarjuara # indonesiajuara https : //t.co/mhipqscir3 jutaan bakau mangrove ditanam pantai utara jabar . upaya kab . subang , sbg upaya alami mempertahankan daratan hilang abrasi air laut akibat disrupsi pemanasan global . # jabarjuara # indonesiajuara https : //t.co/sqtj4jey1b desa tinggal potensi bidang pertanian perikanan ? apapun potensinya , yuk daftarkan desa # sayembaradesadigital program diharapkan memperluas & amp ; meningkatkan desa mandiri jabar . buruan daftar ! lengkapnya cek https : //t.co/azq1dtcw5w ya ! https : //t.co/dvotlpy0ap melepas kontingen jabar festival olahraga rekreasi masyarakat nasional vi 2021 ( fornas vi ) sumatera selatan 2022 . semoga membawa nama jabar aspek memancarkan sifat asli orang jabar yg someah hade ka semah . # jabarjuara # indonesiajuara -admin- https : //t.co/tqqhes4cbi keseruan pagi , pembukaan kejuaraan tenis meja nasional adhyaksa open 2022 . atlet dadakan , kalah pasangan asep m mulyana skor 11-9 & amp ; 11-5 , sportivitas junjung . wilujeng asep ! # jabarjuara # indonesiajuara -admin- https : //t.co/aqsrhoi1il pengusaha muda , ikuti lomba wirausaha muda berprestasi jawa barat 2022 ‚ú® pendaftaran dibuka 20 juni - 20 juli 2022 . hayuk , daftarkan https : //t.co/jqlaplkxo3 üòâ @ disporajabar # jabarjuara # pemudajuara # wirausahamudajabar https : //t.co/e2xthxrlie ketua tim penggerak pkk jabar @ ata_lia , menghadiri acara kesatuan gerak pkk yg 50 tahun.tema yg diangkat kali berbakti bangsa , berbagi sesama.semoga pkk indonesia maju & amp ; memberdayakan masyarakat . # jabarjuara # indonesiajuara -admin- https : //t.co/kcotjmkrcy alhamdulillah berkesempatan menyapa calon haji kloter 34 jabar upt asrama haji bekasi , arahan & amp ; semangat jemaah . insyaallah haji & amp ; hajjah yg mabrur & amp ; mabrurah . aamiin . # jabarjuara # indonesiajuara -admin- https : //t.co/ag6nsl5qr5 insya allah pergi selamat pulang selamat , haji hajjah mabrur mabrurah . aamiin . # jabarjuara # indonesiajuara https : //t.co/ebym7qdarg alhamdulillah allah kemudahan . lokasi penginapan jemaah haji jabar berjarak maksimum 1,500 m masjidil haram & amp ; kabah . pagi melepas keberangkatan rombongan haji jabar embarkasi kota bekasi arahan & amp ; semangat kpd jemaah . https : //t.co/fwar1hegxj 17,000an jemaah haji jawa barat berangkat . pemprov jawa barat berusaha melayani semaksimal jemaah haji aman nyaman menunaikan ibadah haji disana . # jabarjuara # indonesiajuara https : //t.co/sybndkiusw bersyukurlah berhaji 2022 . menunggu tua berhaji rejeki kesempatan pergi usia muda . # jabarjuara # indonesiajuara https : //t.co/gxxdmy3efi ngantri 32 antrian menunggu berhaji mendaftar . sungguh penantian . kuota terbatas , berhaji . # jabarjuara # indonesiajuara https : //t.co/javanv2wn6 jagalah alam , alam menjaga . semangat menata membangun , aman sejahtera . hatur nuhun . # jabarjuara # indonesiajuara https : //t.co/e04ziszufq berduka cita korban meninggal dunia . insya allah negara hadir proses tanggap darurat rekonstruksi kehidupan . bencana silih berganti , semangat kemanusiaan hilang mati . # jabarjuara https : //t.co/2souuzbrqq inspeksi , koordinasi membersamai korban banjir bandang kecamatan pamijahan leuwiliang kab bogor minggu pagi . dana bantuan pemprov jabar tahap tanggap darurat . # jabarjuara # indonesiajuara https : //t.co/m27df1uafu , semoga kopi-kopi terbaik menguasai dunia coffee diplomacy . hatur nuhun . https : //t.co/kor6gte20l negeri cappucino , tim pemprov jabar meluaskan pasar kopi jabar mendunia . pameran kopi dunia milan . ‚Äú i like it . you make a lot of people happy with their first time liberica , ‚Äù si teteh , mawar yg mencoba salah kopi terbaik yg bawa . https : //t.co/iqjlxr8l7b warga jawa barat indonesia kreatif jago gambar . ikuti `` jabart drawing competition 2022 '' in collaboration with wacom . peserta lolos kurasi langsung dinilai . link pendaftaran : https : //t.co/qtbrp5z7vx https : //t.co/z3phnjee2t terima kasih kpd sahabat/teman eril & amp ; masyarakat yg gerakan berbagi kebaikan & amp ; keberkahan . terima kasih yg mencipta lagu eril & amp ; puluhan puisi yg menyayangi eril . menyapa persatu . üôèüôè https : //t.co/ymuqpzzkrd hbd a eril , yg kasih hadiah istimewa lahirmu . mas bony s & amp ; friends bikin khusus animasi tentangmu & amp ; bunga surga . bang abdul like coffee spesial bikin lagu mengenangmu ‚Äú you will always be my home ‚Äù . a very beautiful song . kang teguh bikin editannya keren . https : //t.co/zxxqpiwoby 108 pemuda pemudi hebat . 2032 pendaftar , lantik & amp ; kukuhkan duta pariwisata jabar swj ambassador . tugasnya promosi & amp ; mengedukasi pariwisata ig , twitter dll healing & amp ; merekam keren provinsi yg indah . # jabarjuara # indonesiajuara https : //t.co/cm3ljbowwi investasi . panennya insya allah hadir , munculnya individu-individu karya-karya juara . cetak biru membawa jabar juara ekonomi kreatif yg membawa kesejahteraan rakyat dgn . # jabarjuara # indonesiajuara https : //t.co/oc7e9q5czd negara hadir . menghadirkan fasilitas bergaul berkumpul orang-orang kreatif bentuk creative center resmikan sumedang . https : //t.co/2dmjmkhcoz orang kreatif ? bergaullah orang-orang kreatif terbawa kreatif . orang pintar optimis ? bergaullah pintar optimis , terbawa pintar optimis . https : //t.co/stssaa79fw berduka cita korban banjir bandang melanda desa cibunian , kecamatan pamijahan , kabupaten bogor rabu 22 juni 2022 . doa jabar korban banjir bandang , semoga senantiasa ketabahan kekuatan . aamiin . # jabarjuara # indonesiajuara -admin- https : //t.co/5wi2mbfeom sapi divaksinasi melawan penyakit mulut kuku menyerang ternak minggu dilaksanakan jawa barat . ternak sakit diobati , sehat divaksin booster , haknya melawan penyakit covid manusia . https : //t.co/ibyvie5vge insya allah , idul qurban terselenggara aman nyaman ternak sehat kurban ditandai keping kuning kuping barcode data kesehatan ternak . hatur nuhun . melantik kepengurusan forum kewaspadaan masyarakat ( fkdm ) jabar periode 2022-2025 . sukses selamat bertugas . semoga fkdm organisasi yg sensitif , berpartisipasi menjaga memelihara kondusifitas jawa barat . # jabarjuara # indonesiajuara -admin- https : //t.co/qkmy50ca9j minggu hidup semangat ananda eril menyertai langkah napas . beribu hatur nuhun haturkan . *makam eril terbuka jam 8.00-17.00 . mohon menjaga adab berziarah . haturkan terima kasih mendoakan menyelenggarakan shalat gaib shalat jenazah , sebutkan persatu . semoga allah membalas kebaikan saudara ganjaran berlipat . haturkan terima kasih mengirimkan ucapan bela sungkawa mengirim bunga duka cita sebutkan persatu . ucapkan terima kasih membantu pencarian pemakaman eril sebutkan persatu . https : //t.co/1bj0do1rmd bismillah , 7 eril dimakamkan , resmi menutup rangkaian kegiatan doa takziah publik almarhum ananda eril . https : //t.co/xjlhvwv354 yth asni lubuk tongga , sumatera barat , doa sekeluarga bandung , semoga allah swt mengabulkan doa . semoga ananda ikhsan ditemukan & amp ; bertemu lg kondisi apapun yg terbaik yg allah tetapkan . semoga dikuatkan lahir batin . https : //t.co/sviffrkvr4 bahagia senin kegiatan meresmikan sumedang creative center . gedung investasi jangka mewadahi kreativitas . insyaallah hasilnya dipanen yg . # jabarjuara # indonesiajuara -admin- https : //t.co/ezigton2lp pengumuman hasil seleksi ppdb . mengambil hasil pengumumannya sekolah tujuan ( sekolah mendaftar ) website ppdb https : //t.co/0c5raa3shc . semoga berhasil ! `` sekolah merdeka jabar juara '' ! https : //t.co/vwfudl0muj alhamdulillah makam alm ananda eril , kemarin diziarahi didoakan habib muhammad lutfi bin yahya . terima kasih berkenan mendoakan . jazakallah . https : //t.co/7izlu8oor9 selamat berhari minggu . lupa bergembira berbahagia cinta . salam sehat . https : //t.co/fz37qqmgpg permainan tepuk tangan pelajari khusus bonding dgn zara . jgn lupa rayakan momen anak dgn sederhana memori indah . gak dgn mewah-mewah . selamat pekan . belanjakan dgn yg mencintai . https : //t.co/1jiqs8cgf6 @ ulniania üôè menghadiri wisuda camillia laetitia azzahra sman 3 bandung , sambutan alumni yg masuk angkatan 1988 lulus 1990 . selamat & amp ; sukses ! kuasai perkembangan teknologi & amp ; ekonomi digital , cemerlang . # jabarjuara # indonesiajuara -admin- https : //t.co/ykgqbbwoo4\"]\n"
     ]
    }
   ],
   "source": [
    "# extraction\n",
    "\n",
    "tweet_feat_bow_stop = []\n",
    "\n",
    "d = ' '.join(df_tweet['tweet'])\n",
    "\n",
    "tokens = nltk.word_tokenize(d)\n",
    "tokens_stop = []\n",
    "\n",
    "for t in tokens:\n",
    "    t = t.lower()\n",
    "\n",
    "    if t not in stopword_set:\n",
    "        tokens_stop.append(t)\n",
    "\n",
    "tweet_feat_bow_stop.append(' '.join(tokens_stop))\n",
    "\n",
    "print(tweet_feat_bow_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# vectorization\n",
    "\n",
    "tweet_feat_bow_stop_vec = get_vec(tweet_feat_bow_stop_vectorizer, tweet_feat_bow_stop)\n",
    "\n",
    "print(tweet_feat_bow_stop_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pria'], dtype='<U6')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../model/'\n",
    "model = pickle.load(open(path + 'Ablation bio_Gradient Boosting_train_test_split.sav', 'rb'))\n",
    "X = np.hstack((color_feat, ia_with_dict, username_feat_char, network_feat_abl_likes, behavior_feat_abl_mention, socio_feat_abl_verb_count, tweet_feat_bow_stop_vec))\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
